{
  "metadata": {
    "name": "task_6",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n# ls /home/aleksandr_zhukov8/\n# hadoop fs -put /home/aleksandr_zhukov8/small_movies_10.csv /user/zeppelin\n# hadoop fs -ls /user/zeppelin\n# sudo mv /home/aleksandr_zhukov8/small_movies_10.csv ~/home/aleksandr_zhukov8/tiny_movies.csv\n# cat /home/aleksandr_zhukov8/small_movies_10k.csv | head -n 10\n# cat /home/aleksandr_zhukov8/small_ratings.csv | head -n 5"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n# нормально работать через зеппелин отказывается\ndownload(){\n    help(){\n    echo \u0027\u0027\u0027Usage: download [OPTION]\n    Download specified dataset and unarchive it.\n    \n      -s, --size            dataset size, must be in small|medium|huge\n      -d, --destination     directory path to save to\n      -h, --help            display this help message\n    \n    Examples:\n      download -s small -d ./small_dataset/\n      download --destination ../huge_dataset/ --size huge\n    \u0027\u0027\u0027\n    }\n    \n    # default values\n    DESTINATION\u003d\"./\"\n    SIZE\u003d\"small\"\n    LINK\u003d\"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n    \n    while [[ $# -gt 0 ]]; do\n      key\u003d\"$1\"\n    \n      case $key in\n        -s|--size)\n          SIZE\u003d\"$2\"\n          shift 2\n          ;;\n        -d|--destination)\n          DESTINATION\u003d\"$2/\"\n          shift 2\n          ;;\n        -h|--help)\n          help\n    \t  exit\n          ;;\n      esac\n    done\n    \n    case $SIZE in\n      small)\n        LINK\u003d\"https://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n        ;;\n      medium)\n        LINK\u003d\"https://files.grouplens.org/datasets/movielens/ml-latest.zip\"\n        ;;\n      huge)\n        LINK\u003d\"https://files.grouplens.org/datasets/movielens/ml-20mx16x32.tar\"\n        ;;\n      *)\n        echo \"${SIZE}, only small|medium|huge size is valid, use --help for more\"\n        exit\n        ;;\n    esac\n    \n    echo \"$(curl -o ${DESTINATION}dataset.zip ${LINK})\"\n    echo \"$(unzip -j ${DESTINATION}dataset.zip -d ${DESTINATION})\"\n    echo \"$(rm ${DESTINATION}dataset.zip)\"\n    hadoop fs -put $DESTINATION/* /user/$USER\n}\n\ndownload -s small -d ."
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nimport re\nimport csv\nimport datetime\nfrom pprint import pprint as pp\nfrom collections import namedtuple\nfrom typing import Iterator, NamedTuple, Tuple, Union\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndef read_data(path: str):\n    try:\n        raw_data \u003d sc.textFile(path)\n        header \u003d raw_data.first()\n        rdd_data \u003d raw_data.filter((lambda row: row!\u003dheader))\n    except Exception as e:\n        print(e)\n    return rdd_data, header"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nrdd_movies, header_movies  \u003d read_data(\"small_movies_10k.csv\")\nrdd_ratings, header_ratings \u003d read_data(\"small_ratings.csv\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nprint(rdd_movies.first(), rdd_ratings.first(), sep\u003d\u0027\\n\u0027)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndef normalize_movie(line):\n    try:\n        reader \u003d csv.reader([line])\n        id, title, genres \u003d next(reader)\n    except:\n        return None\n    \n    search_result \u003d re.match(r\u0027(.*)[ ]\\((\\d{4})\\)$\u0027, title)\n    \n    genres \u003d genres.split(\u0027|\u0027)\n    title \u003d search_result.group(1) if search_result else \u0027\u0027\n    year \u003d int(search_result.group(2)) if search_result else 0\n    \n    return [(int(id), (title, year, genre)) for genre in genres]\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndef normalize_rating(line):\n    try:\n        reader \u003d csv.reader([line])\n        _, movie_id, rating, _ \u003d next(reader)\n    except:\n        return None\n\n    return int(movie_id), [float(rating), 1]"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndef movie_filter(line: Tuple[int, Tuple[str, int, str]], filter_rules: NamedTuple) -\u003e bool:\n    \u0027\u0027\u0027\n    Recieves movie and filter rules, returns True if passed movie\n    meets these rules and false otherwise.\n    \u0027\u0027\u0027\n    def year_filter(year_from: int, year_to: int, year: int) -\u003e bool:\n        \u0027\u0027\u0027\n        Returns True if passed movie\u0027s release year matches given interval,\n        False otherwise.\n        \u0027\u0027\u0027\n        if year_from and year_to:\n            return year_from \u003c\u003d year \u003c\u003d year_to\n\n        elif year_from and not year_to:\n            return year \u003e\u003d year_from\n\n        elif not year_from and year_to:\n            return year \u003c\u003d year_to\n        else:\n            return True\n\n    def regex_filter(regexp: str, title: str) -\u003e bool:\n        \u0027\u0027\u0027\n        Returns True if passed movie\u0027s title matches regular expression,\n        False otherwise.\n        \u0027\u0027\u0027\n        if regexp:\n            return True if re.search(regexp, title, re.IGNORECASE) else False\n        else:\n            return True\n\n    def genre_filter(request_genres: str, genre: str) -\u003e bool:\n        \u0027\u0027\u0027Returns True if requested genre expression matches film\u0027s genre, False otherwise.\u0027\u0027\u0027\n        if not request_genres:\n            return True\n\n        request_genres \u003d request_genres.lower().split(\u0027|\u0027)\n        genre \u003d genre.lower()\n        return genre in request_genres\n\n    # (1, (\u0027Toy Story\u0027, 1995, \u0027Adventure\u0027))\n    title \u003d line[1][0]\n    year \u003d  line[1][1]\n    genre \u003d line[1][2]\n\n    match_genre \u003d genre_filter(filter_rules.genres, genre)\n    match_year \u003d year_filter(filter_rules.year_from, filter_rules.year_to, year)\n    match_regexp \u003d regex_filter(filter_rules.regex, title)\n\n    return True if match_genre and match_year and match_regexp else False"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nRule \u003d namedtuple(\u0027Rule\u0027, [\u0027regex\u0027, \u0027genres\u0027, \u0027year_from\u0027, \u0027year_to\u0027])\nrules \u003d Rule(None, None, None, None)\n\npp(rdd_movies \\\n    .flatMap(normalize_movie) \\\n    .filter(lambda line: movie_filter(line, rules)) \\\n    .filter(lambda line: line[1]!\u003d\u0027\u0027) \\\n    .take(10))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nprint(rdd_movies.take(3), rdd_movies.flatMap(normalize_movie).take(3), \u0027\u0027, sep\u003d\u0027\\n\u0027);\nprint(rdd_ratings.take(3), rdd_ratings.map(normalize_rating).take(3), sep\u003d\u0027\\n\u0027);"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n# sum ratings and counts accordingly\n# divide sum of ratings on their count\nrdd_ratings \\\n    .map(normalize_rating) \\\n    .reduceByKey(lambda a, b: [a[0]+b[0], a[1]+b[1]]) \\\n    .mapValues(lambda value: value[0]/value[1]) \\\n    .sortByKey() \\\n    .take(5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nrules \u003d Rule(None, None, None, None)\n\nrdd_normalized_and_filtered_movies \u003d rdd_movies \\\n    .flatMap(normalize_movie) \\\n    .filter(lambda movie: movie_filter(movie, rules))\n\nrdd_average_ratings \u003d rdd_ratings \\\n    .map(normalize_rating) \\\n    .reduceByKey(lambda a, b: [a[0]+b[0], a[1]+b[1]]) \\\n    .mapValues(lambda value: value[0]/value[1])"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\npp(rdd_normalized_and_filtered_movies.take(2))\npp(rdd_average_ratings.take(2))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\nrdd_movies_ratings \u003d rdd_normalized_and_filtered_movies \\\n                        .join(rdd_average_ratings) \\\n                        .mapValues(lambda values: (values[0][0], values[0][1], values[0][2], values[1]))\n\npp(rdd_movies_ratings.sortBy(lambda line: (line[1][1], line[1][3], line[1][2]), ascending\u003dFalse).take(10))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndef reduce(rdd_movies_ratings, N: int\u003dNone):\n    return rdd_movies_ratings \\\n            .sortBy(lambda line: (line[1][2], line[1][1], line[1][3]), ascending\u003dFalse) \\\n            .groupBy(lambda line: line[1][2]) \\\n            .flatMap(lambda line: list(line[1])[:N])\n\nN \u003d 2\npp(reduce(rdd_movies_ratings, N).collect())"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\nhadoop fs -rm ./result.csv/* \u0026\u0026 hadoop fs -rmdir ./result.csv"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndef to_csv(rdd):\n    def flatten(movie):\n        _, (title, year, genre, rating) \u003d movie\n        return genre, title, year, round(rating, 1)\n\n    rdd \u003d rdd \\\n            .map(flatten) \\\n            .sortBy(lambda line: (line[0], -line[2], -line[3]), ascending\u003dTrue) \\\n            .map(lambda line: \u0027,\u0027.join(map(str, line)))\n    \n    rdd_header \u003d sc.parallelize([\"genre,title,year,rating\"])\n    return rdd_header.union(rdd)\n\nrdd_result \u003d reduce(rdd_movies_ratings, 3)\n\npp(to_csv(rdd_result).collect())\n\nto_csv(rdd_result).saveAsTextFile(\"result.csv\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%sh\n# hadoop fs -ls ./result.csv/\nhadoop fs -cat ./result.csv/* | head -n 50\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark"
    }
  ]
}